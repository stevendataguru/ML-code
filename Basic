import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import nltk
import numpy as np
import re
import pickle 
import bs4 as bs
import urllib.request
import heapq
from nltk.corpus import stopwords
from sklearn.datasets import load_files
%matplotlib inline


#submission: (test is test table)
submission = pd.DataFrame({ 'PassengerId': test['PassengerId'],
                            'Survived': final_preds})
submission.to_csv("submission.csv", index=False)



df = pd.read_
df.head()  /  df.info()  /  df.describe()  /  df.columns
sns.pairplot(df)

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)

# SQL:
import pandas as pd
import pyodbc
server = ""
db = ""
con = pyodbc.connect('DRIVER={SQL Server};SERVER=' + server + ';DATABASE=' + db)
query = '''       '''
df = pd.read_sql(query, con)


# save to csv/excel:
df.to_csv('new.csv')
df.to_excel('new.xlsx')

# read from URL table:
import requests
import pandas as pd

url = 'http://www.ffiec.gov/census/report.aspx?year=2011&state=01&report=demographic&msa=11500'
html = requests.get(url).content
df_list = pd.read_html(html)
df = df_list[-1]
print (df)
df.to_csv('my data.csv')

# import URL:
https://stackoverflow.com/questions/10556048/how-to-extract-tables-from-websites-in-python
https://stackoverflow.com/questions/3969726/attributeerror-module-object-has-no-attribute-urlopen


# SQL Join, group by, pivot:
df=pd.merge(df1,df2,how='inner',left_on=['XXX'],right_on=['YYY'])

newtable = pd.DataFrame(df.groupby(' ')[ ' '].count())       *** beaware of table name, .count()/mean()â€¦

newtable = df.pivot_table(index='the index want to pivot', columns=' ', values=' ')
TableA = TableA.join(TableB['column'])      **data+data


# SQL date filter:
df.loc[(df['column'] == 'value') & (df['other column'] == 'value')]
df.loc[(df['column'].isin('value')]
df['Date'] = pd.to_datetime(df['Date'])  
abc=(df['Date']>= ' ')& (df['Date'])
df.drop(columns=['X', 'C'])   or  df.drop('column',1)
df= df.fillna(0)    

#compare two column values
df1['pre']=np.where(df1['Adjclose']>=df1['Open'],1,0)

# sum col(str) and groupby col
dfnew = df.groupby('Col').apply(lambda x: x.sum())

# Grid:
g = sns.FacetGrid(df,col='XXX')
g.map(plt.hist,'YY')

# Seaborn:
#easy
sns.countplot("Class",data=data)

sns.set_style('whitegrid')
plt.figure(figsize=(11,7))

sns.distplot(table['column'])
plt.xlabel('Age')

sns.jointplot(x='1',y='2',data=df)

sns.distplot(df[df['credit.policy']==1]['fico'],bins=30)
sns.distplot(df[df['credit.policy']==0]['fico'],bins=30)

# sns overall heatmap
corrmat = df.corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True)

# sns most related feature
corrmat = df.corr() #change table name
k = 15 #number of variables for heatmap
cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index   # salesprice is the target, change it
cm = np.corrcoef(train[cols].values.T)  # table name here
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()

# scatterplot
sns.set()
cols = [' ', ' ', ' ']
sns.pairplot(df[cols], size = 2.5)
plt.show();


###NEW BASIC

# check if there is null in table
df.isnull().sum()

# total missing data desc
total = df.isnull().sum().sort_values(ascending=False) #table
percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)  #2 tables
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data

df = df.drop((missing_data[missing_data['Percent'] > 0.85]).index,1)  #drop missing data if needed

# sort
df[[' ',' ']].sort_values(by = ' ', ascending=False)[#:#] # is top #

# drop cols
df.drop("Id", axis = 1, inplace = True)
# drop cell
df = df.drop(df[df['Id'] == 1299].index)

# group by and count/median
a=df.groupby('columns').nunique()
a=df.groupby('columns').median()

# distinct count 
df['column'].value_counts()

#* find all variables in cols
n = df.select_dtypes(include=object)
for c in n.columns:
    print('{:<20}'.format(c), df[c].unique())

(n = train.select_dtypes(include=object)
for c in n.columns:
    print('{:<20}'.format(c), train[c].unique()))

# fill null value
df['column']= df['column'].fillna('S')

# get this col's median disregards null value
df.dropna(subset=['column']).median()     

# get dummies
pd.get_dummies(df ['Col'],drop_first=True)

# change datatype
df['col'] = df.col.astype(float)

# round entire table
df=df.round(decimals=2)

# cols to normalize
cols_to_norm = [ ' ', ' ']   #one braket only
df[cols_to_norm]=df[cols_to_norm].apply(lambda x: (x-x.min()) / (x.max()-x.min()))

# min, max, mean
print(train.describe(include=['number']).loc[['min','max','mean']].T.sort_values('max'))

#random sample
df2 = df[df['Class']==0].sample(n=3000)

#union
dfall = df1.append(df2)

#### reshape pivot table
https://pandas.pydata.org/pandas-docs/stable/reshaping.html

# multi cols condition function
def newage(col):
    Age=col[0]
    Sex=col[1]
    Pclass=col[2]
    if pd.isnull(Age):
        if Sex=='male' and Pclass==1:
            return 37
        if Sex=='male' and Pclass==2:
            return 29
        if Sex=='male' and Pclass==3:
            return 24
        if Sex=='female' and Pclass==1:
            return 36
        if Sex=='female' and Pclass==2:
            return 29
        if Sex=='female' and Pclass==3:
            return 24
    else:
        return Age
        
train['newages']=train[['Age','Sex','Pclass']].apply(newage,axis=1)

# signle col condition function
def embarked(col):
    Embarked = col[0]
    if Embarked=='S':
        return 1
    if Embarked=='C':
        return 2
    if Embarked=='Q':
        return 3
        
train['newembark']=train[['Embarked']].apply(embarked,axis=1)


###put into excel
Submission = pd.DataFrame({ 'Id': test['Id'],
                            'SalesPrice': predictions })
Submission.to_csv("Submission.csv", index=False)
